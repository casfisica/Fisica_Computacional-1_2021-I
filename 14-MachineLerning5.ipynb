{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Learning (Aprendizaje Automático)\r\n",
    "\r\n",
    "## Regresión Polinomial.\r\n",
    "\r\n",
    "La regresión lineal es el algoritmo básico de aprendizaje automático y también el más usado. Pero, ¿qué pasa si su modelo de regresión lineal no puede modelar la relación entre las características y la respuesta? En otras palabras, ¿qué pasa si no tienen una relación lineal? Es en estos casos donde la regresión polinomial podría ser de ayuda.\r\n",
    "\r\n",
    "La regresión polinomial es un caso especial de regresión lineal donde ajustamos una ecuación polinomial a los datos con una relación curvilínea entre la variable objetivo y las variables independientes. En una relación curvilínea, el valor de la variable objetivo cambia de manera no uniforme con respecto al predictor o predictores (Variables independientes).\r\n",
    "\r\n",
    "En Regresión lineal, con una sola característica, tenemos la siguiente ecuación:\r\n",
    "\r\n",
    "$$\r\n",
    "Y=\\theta_0+\\theta_1x\r\n",
    "$$\r\n",
    "\r\n",
    "dónde, $Y$ es el objetivo, $x$ es el predictor, $\\theta_0$ es el sesgo (bias), y $\\theta_1$ es el peso (parámetro) en la ecuación de regresión. Esta ecuación lineal se puede utilizar para representar una relación lineal. Pero, en la regresión polinomial, tenemos una ecuación polinomial de grado $n$ representada como:\r\n",
    "\r\n",
    "$$\r\n",
    "Y=\\theta_0+\\theta_1x+\\theta_2x^2+\\theta_3x^3+...+\\theta_1x\r\n",
    "$$\r\n",
    "\r\n",
    "dónde, $Y$ es el objetivo, $x$ es el predictor, $\\theta_0$ es el sesgo (bias), y $\\theta_i$ son los pesos.\r\n",
    "\r\n",
    "\r\n",
    "__Ejemplo:__ Implementaremos tanto la regresión polinomial como los algoritmos de regresión lineal en un conjunto de datos simple donde tenemos una relación curvilínea entre el objetivo y el predictor y compararemos los resultados para comprender la diferencia entre los dos."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Librerias basicas\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "# Para calcular los errores\r\n",
    "from sklearn import metrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generamos datos de manera aleatoria, que sigan una distribucion cuadratica\r\n",
    "N=100\r\n",
    "#Usando una semilla\r\n",
    "np.random.seed(3)\r\n",
    "x = 100*np.random.random(N)\r\n",
    "y = 50-10*x+x*x + 300*np.random.randn(N)\r\n",
    "x_sort = np.linspace(0, 100, N)\r\n",
    "y_real = 50-10*x_sort+x_sort**2 \r\n",
    "\r\n",
    "# Grafiquemos\r\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\r\n",
    "ax.set(xlabel=\"x\",ylabel=\"y\")\r\n",
    "# los valores de x e y\r\n",
    "ax.plot(x, y,'x',color='red',label='Datos')\r\n",
    "ax.plot(x_sort, y_real,label=r'$y(x)$',linestyle=\":\", linewidth=3)\r\n",
    "ax.legend(fontsize=15)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Importemos el modulo de regresion lineal\r\n",
    "from sklearn.linear_model import LinearRegression\r\n",
    "# Instancio\r\n",
    "regressor = LinearRegression()\r\n",
    "# LLamo al metodo fit y le paso los datos de entrenamiento,\r\n",
    "#uso reshape para darle forma de vector columna.\r\n",
    "regressor.fit(x.reshape(-1,1),y.reshape(-1,1))\r\n",
    "# Ahora miro el intercepto y la pendiente de la linea\r\n",
    "print(f\"b = {regressor.intercept_[0]:.3f}, m = {regressor.coef_[0][0]:.3f}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#La funcion de prediccion\r\n",
    "y_pred=regressor.predict(x.reshape(-1,1))\r\n",
    "#Error\r\n",
    "print(f\"MAE : {metrics.mean_absolute_error(y, y_pred):.4f}\")\r\n",
    "print(f'MSE : {metrics.mean_squared_error(y, y_pred):.3f}')\r\n",
    "print(f'RMSE: {np.sqrt(metrics.mean_squared_error(y, y_pred)):.4f}')\r\n",
    "# Grafiquemos\r\n",
    "fig1, ax1 = plt.subplots(figsize=(10, 6))\r\n",
    "ax1.set(xlabel=\"x\",ylabel=\"y\")\r\n",
    "# los valores de x e y\r\n",
    "ax1.plot(x, y,'x',color='red',label='Datos')\r\n",
    "ax1.plot(x_sort, y_real,label=r'$y(x)$',linestyle=\":\", linewidth=3,color='blue')\r\n",
    "ax1.plot(x, y_pred,label=r'$h_{\\theta}(x)$', linewidth=1,color='green')\r\n",
    "ax1.legend(fontsize=15)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se puede ver facilmente que el modelo de regresión lineal no puede ajustar los datos correctamente y los errores son muy altos.\r\n",
    "\r\n",
    "Ahora, intentemos la regresión polinomial. La implementación de la regresión polinomial es un proceso de dos pasos. Primero, transformamos nuestros datos en un polinomio usando la función `PolynomialFeatures` de `sklearn` y luego usamos regresión lineal para ajustar los parámetros. También podemos automatizar este proceso mediante pipelines, éstas se pueden crear utilizando `Pipeline` de `sklearn`.\r\n",
    "\r\n",
    "__Ejemplo:__"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# El modulo para ajustes polinomiales \r\n",
    "from sklearn.preprocessing import PolynomialFeatures\r\n",
    "# Modulo para el pipeline\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "# Creamos un pipeline y ajustamos los datos con un polinomio de grado 2\r\n",
    "Input=[('Polinomio',PolynomialFeatures(degree=2)),('Modo',LinearRegression())]\r\n",
    "pipe=Pipeline(Input)\r\n",
    "pipe.fit(x.reshape(-1,1),y.reshape(-1,1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Podemos elegir el grado de polinomio en función de la relación entre el resultado y las varibles. El polinomio de 1 grado es una regresión lineal simple; por lo tanto, el valor de grado debe ser mayor que 1.\r\n",
    "\r\n",
    "Con el grado creciente del polinomio, la complejidad del modelo también aumenta, luego, el valor de $n$ debe elegirse con precisión, ya que, si este valor es bajo, el modelo no podrá ajustar los datos correctamente y, si es alto, el modelo se ajustará fácilmente a los datos."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "poly_pred=pipe.predict(x.reshape(-1,1))\r\n",
    "#Error\r\n",
    "from sklearn import metrics\r\n",
    "print(f\"MAE : {metrics.mean_absolute_error(y, poly_pred):.4f}\")\r\n",
    "print(f'MSE : {metrics.mean_squared_error(y, poly_pred):.3f}')\r\n",
    "print(f'RMSE: {np.sqrt(metrics.mean_squared_error(y, poly_pred)):.4f}')\r\n",
    "\r\n",
    "#Ordenamos los datos respecto a las variables\r\n",
    "sorted_zip = sorted(zip(x,poly_pred))\r\n",
    "x_poly, poly_pred = zip(*sorted_zip)\r\n",
    "# Grafiquemos\r\n",
    "fig2, ax2 = plt.subplots(figsize=(10, 6))\r\n",
    "ax2.set(xlabel=\"x\",ylabel=\"y\")\r\n",
    "# los valores de x e y\r\n",
    "ax2.plot(x, y,'x',color='red',label='Datos')\r\n",
    "ax2.plot(x, y_pred,':',label=r'$h_{\\theta_L}(x)$',linewidth=1,color='blue')\r\n",
    "ax2.plot(x_poly, poly_pred,label=r'$h_{\\theta_P}(x)$', linewidth=2,color='green')\r\n",
    "ax2.legend(fontsize=15)\r\n",
    "plt.show()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Es muy notorio que la regresión polinomial es mejor para ajustar los datos que la regresión lineal. Además, debido a un mejor ajuste, los errores de la regresión polinomial son mucho más bajos que el de la regresión lineal.\r\n",
    "\r\n",
    "Otra manera de usar PolynomialFeatures, es sin `pipeline`.\r\n",
    "\r\n",
    "__Ejemplo:__ Usemos la base de datos de salarios"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "# Importamos la base de datos\r\n",
    "dataset = pd.read_csv('DataBases/Salarios.csv')\r\n",
    "X = dataset.iloc[:, 1:2].values\r\n",
    "y1 = dataset.iloc[:, 2].values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\r\n",
    "# Ajustemos con un poplinomio de grado 4\r\n",
    "poly_reg = PolynomialFeatures(degree=4)\r\n",
    "#Transformo los datos para usarlos en polyfit\r\n",
    "X_poly = poly_reg.fit_transform(X)\r\n",
    "#Instancio un objeto LinearRegression\r\n",
    "pol_pred = LinearRegression()\r\n",
    "#Hago el ajuste\r\n",
    "pol_pred.fit(X_poly, y1)\r\n",
    "\r\n",
    "# Grafiquemos\r\n",
    "fig2, ax2 = plt.subplots(figsize=(10, 6))\r\n",
    "ax2.set(xlabel=\"Posición\",ylabel=\"Salario\")\r\n",
    "# los valores de x e y\r\n",
    "ax2.plot(X, y1,\"x\",color='red',label='Datos')\r\n",
    "ax2.plot(X,pol_pred.predict(X_poly) ,label=r'$h_{\\theta_P}(x)$', linewidth=3,color='green')\r\n",
    "ax2.legend(fontsize=15)\r\n",
    "plt.show()\r\n",
    "\r\n",
    "from sklearn import metrics\r\n",
    "print(f\"MAE : {metrics.mean_absolute_error(y1, pol_pred.predict(X_poly)):.4f}\")\r\n",
    "print(f'MSE : {metrics.mean_squared_error(y1, pol_pred.predict(X_poly)):.3f}')\r\n",
    "print(f'RMSE: {np.sqrt(metrics.mean_squared_error(y1, pol_pred.predict(X_poly))):.4f}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Pero, ¿y si tenemos más de una caracteristica? Para 2 caracteristicas, la ecuación de la regresión polinomial se convierte en:\r\n",
    "\r\n",
    "$$\r\n",
    "Y=\\theta_0+\\theta_1x_1+\\theta_2x_2+\\theta_3x_1x_2+\\theta_4x^2_1+\\theta_5x^2_2\r\n",
    "$$\r\n",
    "\r\n",
    "En general para $n$ predictores, la ecuación incluye todas las combinaciones posibles de polinomios de diferentes órdenes. Esto se conoce como regresión polinomial multidimensional.\r\n",
    "\r\n",
    "Pero hay un problema importante con la regresión polinomial multidimensional: la multicolinealidad, esto es la interdependencia entre los predictores en un problema de regresión multidimensional. Esto impide que el modelo se ajuste correctamente un conjunto de datos."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "ef4c800377bbf5e33c22b32317d08627756e48ff809c13c8c6fbaacd1e58e059"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}