{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Learning (Aprendizaje Automático)\r\n",
    "## Clasificación\r\n",
    "\r\n",
    "¿Qué es la clasificación?\r\n",
    "Los algoritmos de aprendizaje automático supervisados definen modelos que capturan las relaciones entre los datos. La clasificación es un área de aprendizaje automático supervisado que intenta predecir a qué clase o categoría pertenece una entidad, en función de sus características.\r\n",
    "\r\n",
    "## Clasificación Binaria\r\n",
    "\r\n",
    "Suponga que, en nuestro problema de las notas de curso, queremos intentar predecir si el estudiante gana o no el curso, con base en algunas de las características mencionadas. Esto es lo que llamamos un problema de clasificación. Para intentar encontrar una hipótesis a un problema de este tipo, podríamos usar regresión lineal o polinomial (es el único que manejamos) y mapear todas las predicciones mayores de un cierto valor como 1 y todas las menores como 0. Sin embargo, este método no funciona bien porque la clasificación no es en realidad una función de este tipo.\r\n",
    "\r\n",
    "El problema de clasificación es como el problema de regresión, excepto que los valores que ahora queremos predecir toman solo una pequeña cantidad de valores discretos. Por ahora, nos centraremos en el problema de clasificación binaria en el que, y puede tomar solo dos valores, 0 y 1.\r\n",
    "\r\n",
    "__Ejemplo:__ Usamos el paquete `sklearn` para generar un conjunto de datos de prueba, y sobre estos haremos el análisis."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Importo make_classification\r\n",
    "from sklearn.datasets import make_classification\r\n",
    "X, y = make_classification(n_features=2, n_redundant=0, \r\n",
    "                           n_informative=2, random_state=1, \r\n",
    "                           n_clusters_per_class=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Grafiquemos\r\n",
    "fig, ax = plt.subplots(1,2,figsize=(14, 6))\r\n",
    "#ax1.set(xlabel=\"Horas de estudio semanal\")\r\n",
    "# los valores de x e y\r\n",
    "ax[0].plot(X[:,0][y==0],y[y==0],'x',color='red',label='Perdio')\r\n",
    "ax[0].plot(X[:,0][y==1],y[y==1],'o',color='blue',label='Gano')\r\n",
    "ax[0].legend(fontsize=15)\r\n",
    "ax[1].plot(X[:,1][y==0],y[y==0],'x',color='red',label='Perdio')\r\n",
    "ax[1].plot(X[:,1][y==1],y[y==1],'o',color='blue',label='Gano')\r\n",
    "ax[1].legend(fontsize=15)\r\n",
    "\r\n",
    "plt.plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Grafiquemos\r\n",
    "fig1, ax1 = plt.subplots(figsize=(10, 6))\r\n",
    "#ax1.set(xlabel=\"Horas de estudio semanal\")\r\n",
    "# los valores de x e y\r\n",
    "ax1.plot(X[:,0][y==0],X[:,1][y==0],'x',color='red',label='Perdio')\r\n",
    "ax1.plot(X[:,0][y==1],X[:,1][y==1],'o',color='blue',label='Gano')\r\n",
    "ax1.legend(fontsize=15)\r\n",
    "plt.plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regresión logística\r\n",
    "\r\n",
    "La hipótesis $h_\\theta(x)$ debe de satisfacer $0 \\leq h_\\theta(x) \\leq 1$, y esto se logra usando la _Función logística_  (también llamada Función sigmoidea)\r\n",
    "\r\n",
    "$$h_\\theta(x) = \\frac{1}{1+e^{\\theta^Tx}}$$\r\n",
    "\r\n",
    "Sabemos que hay muchas funciones continuas que generan valores entre 0 y 1. ¿Por qué elegimos solo la función logística, por qué no cualquier otra? En realidad, existe una clase más amplia de algoritmos denominados _Modelos lineales generalizados_ de los cuales este es un caso especial. \r\n",
    "\r\n",
    "A continuación se ve una gráfica de la función"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def sigmoid(z):\r\n",
    "    '''Funcion Sigmoidea\r\n",
    "        Args:\r\n",
    "            - Z: objeto de de python compatible con numpy\r\n",
    "        Out:\r\n",
    "            - Sigmoidea aplicada a z \r\n",
    "    '''\r\n",
    "    return 1/(1 + np.exp(-z))\r\n",
    "\r\n",
    "z = np.linspace(-10, 10, 100)\r\n",
    "Y = sigmoid(z)\r\n",
    "  \r\n",
    "plt.plot(z, Y)\r\n",
    "plt.xlabel(\"z\")\r\n",
    "plt.ylabel(\"Sigmoid(z)\")\r\n",
    "plt.grid(True)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "La función Sigmoide, que se muestra aquí, asigna cualquier número real al intervalo (0, 1), lo que la hace útil para transformar una función de valor arbitrario en una función más adecuada para la clasificación.\r\n",
    "\r\n",
    "$h_{\\theta}(x)$ se interpreta como la probabilidad de que la salida sea $1$, luego $h_{\\theta}(x)=P(y|x;\\theta)$, que se lee __\"Probabilidad de $y$ dados $x$ parametrizado por $\\theta$\"__\r\n",
    "\r\n",
    "__Ejemplo:__ si $h_\\theta(x) = 0.7$ nos da una probabilidad del $70\\%$ de que nuestra salida sea 1. Nuestra probabilidad de que nuestra predicción sea 0 es solo el complemento de nuestra probabilidad de que sea 1.\r\n",
    "\r\n",
    "## Función de costo para la regresión logística\r\n",
    "\r\n",
    "Para cada algoritmo de aprendizaje automático, necesitamos una función de costo o pérdida, que queremos minimizar para determinar los parámetros óptimos que nos ayudarán a hacer las mejores predicciones. Para la regresión lineal, tuvimos el error cuadrático medio como función de pérdida. Para un problema de clasificación binaria, necesitamos poder generar la probabilidad de que $y$ sea 1, luego podemos determinar la probabilidad de que $y$ sea 0 o viceversa.\r\n",
    "\r\n",
    "La función de costo para la regresión logística esta definida como (loglikelihood)\r\n",
    "$$\r\n",
    "Cost\\left(h_{\\theta}(x),y\\right)=\\begin{cases}\r\n",
    "-\\log(h_{\\theta}(x)) & si~~y=1\\\\\r\n",
    "\\\\\r\n",
    "-\\log(1-h_{\\theta}(x)) & si~~y=0\r\n",
    "\\end{cases}\r\n",
    "$$\r\n",
    "\r\n",
    "o de forma más compacta\r\n",
    "$$\r\n",
    "Cost\\left(h_{\\theta}(x),y\\right)=-y\\log(h_{\\theta}(x))-(1-y)\\log(1-h_{\\theta}(x))\r\n",
    "$$\r\n",
    "\r\n",
    "note que la función de costo se acerca a 0 cuando predecimos correctamente, es decir, cuando $y = 0$ y $h_{\\theta}(x) = 0$ o, $y = 1$ y $h_{\\theta}(x) = 1$, y la función de pérdida se acerca al infinito si predecimos incorrectamente, es decir , cuando $y = 0$ pero $h_{\\theta}(x) = 1$ o, $y = 1$ pero $h_{\\theta}(x) = 1$.\r\n",
    "\r\n",
    "Luego cuando se implementa para una cantidad de datos $m$, se escribe\r\n",
    "\r\n",
    "$$\r\n",
    "J(\\theta)=\\frac{1}{m}\\sum^m_{i=1}[-y^{(i)}\\log(h_{\\theta}(x^{(i)}))-(1-y^{(i)})\\log(1-h_{\\theta}(x^{(i)}))]\r\n",
    "$$\r\n",
    "\r\n",
    "o de manera vectorizada\r\n",
    "\r\n",
    "$$\r\n",
    "J(\\theta)=\\frac{1}{m}(-y^T\\log(h)-(1-y)^T\\log(1-h))\r\n",
    "$$\r\n",
    "\r\n",
    "con $\\log$ función vectorial. A continuación, se define dicha función para el caso ejemplo"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def Cost(y, y_hip):\r\n",
    "    '''Funcion Costo\r\n",
    "    Args:\r\n",
    "        - y ( numpy float array): Vector con los valores del dominio de entrenamiento\r\n",
    "        - y_hip respuesta generada por la hipotesis\r\n",
    "    Out:\r\n",
    "        - Costo \r\n",
    "    '''\r\n",
    "    m=y.shape[0]\r\n",
    "    cost = -np.sum((1/m)*(y.T.dot(np.log(y_hip)))+(1-y).T.dot(np.log(sigmoid(1-y_hip))))\r\n",
    "    return cost"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "__NOTA:__ En el caso de regresión logística es muy importante que todas las variables estén normalizadas, pues van a pasar por la función sigmoide."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def normalize(X):\r\n",
    "    '''Funcion Costo\r\n",
    "    Args:\r\n",
    "    - X ( numpy float array): Matriz con los valores del dominio de entrenamiento\r\n",
    "    Out:\r\n",
    "    - X normalizado \r\n",
    "    '''    \r\n",
    "    # m-> numero de ejemplos de entrenamiento\r\n",
    "    # n-> numero de caracteristicas (variables)\r\n",
    "    m, n = X.shape;\r\n",
    "\r\n",
    "    # Normalizando.\r\n",
    "    for i in range(n):\r\n",
    "        X = (X - X.mean(axis=0))/X.std(axis=0)\r\n",
    "\r\n",
    "    return X"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "La minimización de esta función se hace a través de GD. La derivada de $J$ es la misma que en el caso de la regresión lineal, \r\n",
    "$$\r\n",
    "\\theta_j: = \\theta_j -\\frac{\\alpha}{m}\\sum_{i=1}^{m}(h(\\theta^{(i)}) - y^{(i)}).x^{(i)}_j\r\n",
    "$$\r\n",
    "o en forma Vectorial\r\n",
    "\r\n",
    "$$\r\n",
    "\\theta_j: = \\theta_j-\\frac{\\alpha}{m}X^T(h_\\theta(X)-y)\r\n",
    "$$\r\n",
    "\r\n",
    "pero debemos de tener en cuenta que $h_\\theta(x) = \\frac{1}{1+e^{x\\theta}}$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def GDRL(X_train,Y_train,Theta,alpha=0.001,steps=100):\r\n",
    "    '''Gradient Descent para regresion logistica\r\n",
    "        Args:\r\n",
    "            - Theta (array float): parámetros iniciales\r\n",
    "                Ejem: Theta = np.ones([1,2])\r\n",
    "            - X_train (numpy float arrays): Matriz mxn con los valores del rango de entrenamiento\r\n",
    "                n -> numero de caracteristicas, m->numero de ejemplos de entrenamiento\r\n",
    "            - Y_train( numpy float array): Vector con los valores del dominio de entrenamiento \r\n",
    "            - alpha (float): learning rate\r\n",
    "            - steps (int): numero de pasos\r\n",
    "        Out:\r\n",
    "            - Valor de Theta actualizado a la ultima iteracion (array float)\r\n",
    "            - Historico del Thetha\r\n",
    "            - Historico del costo\r\n",
    "    '''\r\n",
    "    n=X_train.shape[1]\r\n",
    "    m=X_train.shape[0]\r\n",
    "    #si entra como vector fila, lo pasa a columna (con n+1 filas)\r\n",
    "    Theta_0=Theta.reshape(n+1,1)\r\n",
    "    #si entra como vector fila, lo pasa a columna (con m filas)\r\n",
    "    y=Y_train.reshape(m,1)\r\n",
    "    #Normalizo los datos\r\n",
    "    Xnom=normalize(X_train)\r\n",
    "    #Anadimos los 1 correspondientes a los X_0\r\n",
    "    X=np.c_[np.ones(m),Xnom]\r\n",
    "    #Historial de Theta\r\n",
    "    HistoThetaArray=[]\r\n",
    "    #Primer valor\r\n",
    "    HistoThetaArray.append(Theta_0)\r\n",
    "    #historial Costo\r\n",
    "    histoCostArray=[]\r\n",
    "    #For por cada paso\r\n",
    "    for i in range(steps):\r\n",
    "        #Calculo el costo y al historial\r\n",
    "        histoCostArray.append(Cost(y,sigmoid(X.dot(Theta_0))))\r\n",
    "        #Actualizo los valores de theta\r\n",
    "        Theta_0 = Theta_0 - (alpha/m)*np.dot(X.T,sigmoid(X.dot(Theta_0)) - y)\r\n",
    "        #Anado al historial\r\n",
    "        HistoThetaArray.append(Theta_0)\r\n",
    "        \r\n",
    "    HistoTheta=np.array(HistoThetaArray)\r\n",
    "    histoCost=np.array(histoCostArray)\r\n",
    "        \r\n",
    "    return Theta_0,HistoTheta,histoCost"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Theta = np.array([0,0,0])\r\n",
    "Theta_0,HistoTheta,histoCost = GDRL(X,y,Theta,alpha=0.01,steps=10000)\r\n",
    "#histoCost"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig2, ax2 = plt.subplots(nrows=1, ncols=4,figsize=(24,6))\r\n",
    "ax2[0].plot(histoCost,'--',color='red');\r\n",
    "ax2[0].grid(True)\r\n",
    "ax2[0].set(ylabel=\"Cost(z)\",xlabel=\"iter\")\r\n",
    "ax2[1].plot(HistoTheta[:,0],'--',color='green');\r\n",
    "ax2[1].set(ylabel=r\"$\\theta_0$\",xlabel=\"iter\")\r\n",
    "ax2[1].grid(True)\r\n",
    "ax2[2].plot(HistoTheta[:,1],'--',color='gray');\r\n",
    "ax2[2].set(ylabel=r\"$\\theta_1$\",xlabel=\"iter\")\r\n",
    "ax2[2].grid(True)\r\n",
    "ax2[3].plot(HistoTheta[:,2],'--');\r\n",
    "ax2[3].set(ylabel=r\"$\\theta_2$\",xlabel=\"iter\")\r\n",
    "ax2[3].grid(True)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Contornos de decisión\r\n",
    "\r\n",
    "Si decidimos que si $h_\\theta(x) \\leq 0.5$ entonces $y=1$ entonces tendremos que \r\n",
    "\r\n",
    "$$\\theta^Tx\\leq 0 \\Rightarrow y=1$$\r\n",
    "\r\n",
    "$$\\theta^Tx > 0 \\Rightarrow y=0$$\r\n",
    "\r\n",
    "A la función que genera esta escogencia de parámetros la llamaremos contornos de decisión. Luego una vez encontrado $\\theta$ podemos ver este contorno (al menos para dos caracteristicas)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x1 = [min(X[:,0]), max(X[:,0])]\r\n",
    "m = -Theta_0[1]/Theta_0[2]\r\n",
    "c = -Theta_0[0]/Theta_0[2]\r\n",
    "x2 = m*x1 + c\r\n",
    "    \r\n",
    "# Grafiquemos\r\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\r\n",
    "#ax1.set(xlabel=\"Horas de estudio semanal\")\r\n",
    "# los valores de x e y\r\n",
    "\r\n",
    "plt.xlim([X[:,0].min(), X[:,0].max()])\r\n",
    "plt.ylim([X[:,1].min(), X[:,1].max()])\r\n",
    "\r\n",
    "ax.plot(X[:,0][y==0],X[:,1][y==0],'x',color='red',label='Perdio')\r\n",
    "ax.plot(X[:,0][y==1],X[:,1][y==1],'o',color='blue',label='Gano')\r\n",
    "plt.xlabel(r\"$X_1$\")\r\n",
    "plt.ylabel(r\"$X_2$\")\r\n",
    "plt.title('Contorno')\r\n",
    "plt.plot(x1, x2, 'y-')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "ef4c800377bbf5e33c22b32317d08627756e48ff809c13c8c6fbaacd1e58e059"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}