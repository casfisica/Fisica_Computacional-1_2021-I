{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Learnin (Aprendizage Automatico)\r\n",
    "\r\n",
    "Dos definiciones muy famosas de Machine Learning (ML), la primera informal y más antigua:\r\n",
    "\r\n",
    "\"el campo de estudio que da a las computadoras la capacidad de aprender sin ser programadas explícitamente\". \r\n",
    "_Arthur Samuel_\r\n",
    "\r\n",
    "Y la segunda más moderna: \r\n",
    "\r\n",
    "\"Se dice que un programa de computadora aprende de la experiencia E con respecto a alguna clase de tareas T y medida de desempeño P, si su desempeño en las tareas de T, medido por P, mejora con la experiencia E. \"\r\n",
    "_Tom Mitchell_\r\n",
    "\r\n",
    "Ejemplo: Mario bross.\r\n",
    "\r\n",
    "E = la experiencia de jugar muchos juegos.\r\n",
    "\r\n",
    "T = la tarea de jugar.\r\n",
    "\r\n",
    "P = la probabilidad de que el programa llegue al último nivel.\r\n",
    "\r\n",
    "En general, cualquier problema de aprendizaje automático se puede asignar a una de dos clasificaciones amplias:\r\n",
    "\r\n",
    "__Arendizaje supervisado__ y __aprendizaje no supervisado__."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Arendizaje supervisado\r\n",
    "Se le da al algoritmo un conjunto de respuestas correctas, comunmente llamado conjunto de entrenamiento, y luego se le pide que prediga el comportamiento de datos desconocidos, teniendo la idea de que existe una relación entre la entrada y la salida.\r\n",
    "\r\n",
    "Los problemas de aprendizaje supervisado se clasifican en problemas de \"_regresión_\" y \"_clasificación_\". En un problema de regresión, estamos tratando de predecir resultados dentro de una salida _continua_, lo que significa que estamos tratando de asignar variables de entrada a alguna función continua. En un problema de clasificación, en cambio, estamos tratando de predecir los resultados en una salida discreta. En otras palabras, estamos tratando de mapear variables de entrada en categorías discretas.\r\n",
    "\r\n",
    "Ejemplos:\r\n",
    "\r\n",
    "Si tenemos las notas del curso de \"Física Computacinal\" de los semestres inmediatamente anteriores, en función de las horas que estudian para el curso, podemos intentear predecir su las notas de algunos estudiantes este semestre.\r\n",
    "Podríamos convertir este ejemplo en un problema de clasificación, cambiando la nota por la etiqueta de si ganó el curso o no."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Representación del modelo\r\n",
    "En el caso de nuestro ejemplo de las notas, los datos de entrenamiento se presentan así\r\n",
    "| H   | N   |\r\n",
    "|-----|-----|\r\n",
    "| 4.2 | 2.3 |\r\n",
    "| 7.2 | 2.6 |\r\n",
    "| 1.1 | 0.1 |\r\n",
    "| 3.1 | 1.1 |\r\n",
    "| 1.4 | 0.8 |\r\n",
    "| ... | ... |\r\n",
    "\r\n",
    "Usaremos $x^{(i)}$ para denotar las variables de \"input\" (Horas de estudio por ejemplo), también llamadas características de entrada, e $y^{(i)}$ para denotar la \"output\" o la variable objetivo que estamos tratando de predecir (nota en el ejemplo). Un par $(x^{(i)}, y^{(i)})$, lo llamamos ejemplo de entrenamiento y $\\{(x^{(i)}, y^{(i)}); i = 1,. . . , m)\\}$ se denomina conjunto de entrenamiento. por ejemplo el par $(x^{(2)},y^{(2)})$ es $(7.2,2.6)$, o $y^{(3)} = 0.1$ \r\n",
    "\r\n",
    "\r\n",
    "Para describir el problema de aprendizaje supervisado de manera un poco más formal, nuestro objetivo es, dado un conjunto de entrenamiento, aprender una función $h: X \\rightarrow Y$ de modo que $h (x)$ sea un predictor \"bueno\" para el valor correspondiente de $y$. Por razones históricas, esta función $h$ se denomina _hipótesis_. \r\n",
    "\r\n",
    "El proceso es visto gráficamente:\r\n",
    "\r\n",
    "<!-- begin figure -->\r\n",
    "<div id=\"\"></div>\r\n",
    "<img src=\"Fig/RepresentacionModelo.png\" width=300>\r\n",
    "<p></p>\r\n",
    "<!-- end figure -->"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Funciones de Costo\r\n",
    "\r\n",
    "Por simplicidad la primera hipotesis que usaremos será, la función lineal (regreción lineal). \r\n",
    "$$h_{\\theta}(x)=\\theta_0+\\theta_1 x$$\r\n",
    "\r\n",
    "Como ejemplo miremos los puntos de datos de la siguiente gráfica (que generamos usando una función lineal y números aleatorios). Lo que queremos es encontrar una función $h_{\\theta}$ tal que prediga correctamente la salida, dado un dato de entrada, que en este caso será dada una cantidad de horas estudiadas, que nota tendrá el estudiante."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Importamos los modulos necesarios\r\n",
    "from random import choice\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Creamos un conjunto de datos\r\n",
    "N=30\r\n",
    "#Usando una semilla\r\n",
    "np.random.seed(1)\r\n",
    "x = 10*np.random.random(N)\r\n",
    "y = (0.2+0.4*x) + 0.4*np.random.randn(N)\r\n",
    "y2 = 0.2+0.4*x\r\n",
    "# Grafiquemos\r\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\r\n",
    "ax.set(xlabel=\"Horas\",ylabel=\"Nota\")\r\n",
    "# los valores de x e y\r\n",
    "ax.plot(x, y,'x',color='red',label='Datos')\r\n",
    "ax.plot(x, y2,label=r'$h_{\\theta}(x)$' )\r\n",
    "ax.legend(fontsize=15)\r\n",
    "ax.set_xlim([0.0, 10.0])\r\n",
    "ax.set_ylim([0.0, 5.0])\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "En este caso tenemos la una posible hipótesis, graficada en azul. Ahora la pregunta es: ¿Cómo sabemos que esta hipotesis es la mejor?\r\n",
    "\r\n",
    "Podemos medir la precisión de nuestra función hipótesis usando una función de costo. Esto toma una diferencia promedio (en realidad, una versión más elegante de un promedio) de todos los resultados de la hipótesis con entradas de $x$ y la salida real de $y$.\r\n",
    "\r\n",
    "$$J(\\theta_0, \\theta_1) = \\dfrac{1}{2m} \\sum_{i = 1}^m\\left(\\hat{y}_{i}-y_{i}\\right)^2= \\dfrac{1}{2m} \\sum_{i = 1}^m\\left( h_{\\theta}(x_{i})-y_{i}\\right)^2$$ \r\n",
    "\r\n",
    "Esta función se denomina _\"Función de error cuadrático\"_ o _\"Error al cuadrado medio\"_. La media se reduce a la mitad por conveniencia para el cálculo del __Gradient Desent__ (Descenso de Gradiente).\r\n",
    "\r\n",
    "Si tratamos de pensarlo en términos visuales, nuestro conjunto de datos de entrenamiento está disperso en el plano $x-y$. Estamos tratando de hacer una línea recta (definida por $h_{\\theta}(x)$ que pasa a través de estos puntos de datos dispersos.\r\n",
    "\r\n",
    "Nuestro objetivo es conseguir la mejor línea posible. La mejor línea posible será tal que las distancias verticales cuadradas promedio de los puntos dispersos desde la línea sean las menores. Idealmente, la línea debería pasar por todos los puntos de nuestro conjunto de datos de entrenamiento. En tal caso, el valor de $J(\\theta_0, \\theta_1) $ será 0. \r\n",
    "\r\n",
    "El siguiente ejemplo muestra la situación ideal en la que tenemos una función de costo de 0."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def J(X_train,Y_train,Theta0,Theta1, ax=None):\r\n",
    "    '''Función de costo\r\n",
    "    Args:\r\n",
    "    - theta0_1 (float): parámetros\r\n",
    "    - X_train, Y_train( numpy float array): Vector con los valores de entrenamiento\r\n",
    "    - ax Eje sobre el que hace la media: por defecto None\r\n",
    "    Out:\r\n",
    "    - Valor de J (float)\r\n",
    "    '''\r\n",
    "    #Calculo los puntos predichos por la ecuacion de la linea,\r\n",
    "    # con theta0 y theta 1\r\n",
    "    Y_pred=Theta0+np.dot(Theta1,X_train)\r\n",
    "    return np.square(np.subtract(Y_train,Y_pred)).mean(axis=ax)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Miremos las gráficas de la función $h$ y de $J$ para distintos conjuntos de parámetros (por simplicidad solo cambiare $\\theta_1$) y usaremos el conjunto de datos $\\{(1,1),(2,2),(3,3)\\}$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Datos de entrenamiento\r\n",
    "E = np.array([[1., 2., 3.],[1., 2., 3.]])\r\n",
    "#Creamos los subplots para graficar\r\n",
    "fig, ax = plt.subplots(nrows=3, ncols=3,figsize=(15, 8))\r\n",
    "#Vector para guardar los valores de J\r\n",
    "Errors = [];\r\n",
    "theta1 = [];\r\n",
    "#Usando un for \r\n",
    "for i in range(-3,6):\r\n",
    "    #Para producir los indices de los subplots\r\n",
    "    r, c = (i+3) // 3, (i) % 3\r\n",
    "    #Linea\r\n",
    "    h=lambda x: i*x\r\n",
    "    #Grafico\r\n",
    "    ax[r,c].clear()\r\n",
    "    ax[r,c].plot(E[0],E[1],'x',color='red')#Datos\r\n",
    "    #Hipotesis\r\n",
    "    ax[r,c].plot(E[0],h(E[0]))\r\n",
    "    #Pongo los titulos\r\n",
    "    mse=J(E[0],E[1],Theta0=0,Theta1=i, ax=None)\r\n",
    "    ax[r,c].set_title(f\"J = {mse:.3f}\")\r\n",
    "    theta1.append(i)\r\n",
    "    Errors.append(mse)\r\n",
    "plt.tight_layout()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora miremos la gráfica de $J(\\theta)$, donde podremos encontrar que esfectivamente el mínimo de $J$ se encuentra en cuando el valor del parámetro $\\theta$ es uno."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Grafico\r\n",
    "fig1, ax1 = plt.subplots()\r\n",
    "ax1.plot(theta1, Errors )\r\n",
    "ax1.set(xlabel=r'$\\theta_1$',ylabel=\"J\")\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "En el ejemplo anterior, tenemos una función de costo que solo tiene un parámetro, generalmente (en especial si no son regresiones lineales) las funciones tendran más de un parámetro, y esto puede complicar un analisis gráfico. Lo que se usa en estos casos lo que se quiere es un algoritmo para minimizar la función de costo.\r\n",
    "\r\n",
    "## Gradient Desent(Descenso por gradiente)\r\n",
    "Entonces tenemos nuestra función de hipótesis y tenemos una forma de medir qué tan bien encaja en los datos (funcion de costo). Ahora necesitamos estimar los parámetros en la función de hipótesis. Ahí es donde entra en juego el GD.\r\n",
    "\r\n",
    "Si graficamos la función de hipótesis en función de sus parámetros $\\theta_0$ y $\\theta_0$ (No estamos graficando $x$ e $y$ en sí, sino el rango de parámetros de la función de hipótesis) y el costo resultante de seleccionar un conjunto particular de parámetros. Los puntos en del gráfico serán el resultado de la función de costo usando la hipótesis con esos parámetros $\\theta$ específicos. El siguiente gráfico muestra una configuración de este tipo.\r\n",
    "Tenemos una función de costo, que en general depende de $n$ parámetros $\\theta$, \r\n",
    "\r\n",
    "<!-- begin figure -->\r\n",
    "<div id=\"\"></div>\r\n",
    "<img src=\"Fig/GD.png\" width=600>\r\n",
    "<p></p>\r\n",
    "<!-- end figure -->\r\n",
    "\r\n",
    "Sabremos que hemos tenido éxito cuando nuestra función de costo esté en la parte inferior de los pozos en la gráfica, es decir, cuando su valor sea el mínimo. Las flechas rojas muestran los puntos mínimos en el gráfico.\r\n",
    "\r\n",
    "La forma en que hacemos esto es tomando la derivada de la función de costo. La pendiente de la tangente es la derivada en ese punto y nos dará una dirección hacia la cual movernos. Realizamos pasos hacia abajo en la función de costo en la dirección con el descenso más pronunciado. El tamaño de cada paso está determinado por el parámetro $\\alpha$, que se denomina tasa de aprendizaje (learning rate). Dependiendo de dónde se comience en la gráfica, se podría terminar en diferentes puntos.\r\n",
    "\r\n",
    "El algoritmo de descenso de gradiente es repetir hasta la convergencia:\r\n",
    "\r\n",
    "$$\\theta_i:=\\theta_i-\\alpha\\frac{\\partial}{\\partial\\theta_i}J(\\{\\theta_j\\}^{n}_{j=0}),~~\\text{con}~i=0,1,...,n$$\r\n",
    "\r\n",
    "Tenga en cuenta que \"$:=$\" significa asignación no igualdad y que las dos ecuaciones deben de ser actualizadas simultaneamente."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Creamos un conjunto de datos\r\n",
    "x1 = np.linspace(-3,5)\r\n",
    "y1 = (x1-1)**2\r\n",
    "#Como sabemos la funcion, tenemos su derivada\r\n",
    "Jtheta= lambda x1: (x1-1)**2\r\n",
    "dy1 = lambda x1: 2*(x1-1)\r\n",
    "#Apliquemos el algoritmo, usando un alpha de 0.1 y comenzamos en theta_1=5\r\n",
    "N=15\r\n",
    "thetaVec=np.zeros(N)\r\n",
    "thetaVec[0]=5\r\n",
    "alpha = 0.1\r\n",
    "for k in range(N-1):\r\n",
    "    thetaVec[k+1]=thetaVec[k]-alpha*dy1(thetaVec[k])\r\n",
    "\r\n",
    "# Grafiquemos\r\n",
    "fig2, ax2 = plt.subplots()\r\n",
    "ax2.set(xlabel=r'$\\theta_1$',ylabel=r'$J(\\theta_1)$')\r\n",
    "# J(theta)\r\n",
    "ax2.plot(x1, y1)\r\n",
    "ax2.plot(thetaVec, Jtheta(thetaVec),'x',color='red')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Funcion de Costo__\r\n",
    "Para el caso de la regresión lineal tenemos que la derivadas son\r\n",
    "\r\n",
    "$$\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m}\\sum_{i=1}^{m}(h(\\theta^{(i)}) - y^{(i)}).x_j^{(i)}$$\r\n",
    "\r\n",
    "luego\r\n",
    "\r\n",
    "$$\\theta_0: = \\theta_0 - \\frac{\\alpha}{m} \\sum_{i=1}^{m}(h(\\theta^{(i)}) - y^{(i)})$$\r\n",
    "\r\n",
    "$$\\theta_1: = \\theta_1 -\\frac{\\alpha}{m}\\sum_{i=1}^{m}(h(\\theta^{(i)}) - y^{(i)}).x^{(i)}$$\r\n",
    "\r\n",
    "Usemos, esta tecnica sobre nuestros datos de las Notas de estudiantes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "#Definimos la funcion de Desenso por gradiente\r\n",
    "def GDLR(X_train,Y_train,Theta=[0,0],alpha=0.01,steps=10):\r\n",
    "    '''Gradient Descent Lineal Regression\r\n",
    "    Args:\r\n",
    "    - Theta (array float): parámetros iniciales np.ones([1,2])\r\n",
    "    - X_train, Y_train( numpy float array): Vector con los valores de entrenamiento\r\n",
    "    - alpha (float): learning rate\r\n",
    "    - steps (int): numero de pasos\r\n",
    "    Out:\r\n",
    "    - Valor de Theta actualizado (array float)\r\n",
    "    - Historico del Thetha\r\n",
    "    - Historico del costo\r\n",
    "    '''\r\n",
    "    #Numero de muestras\r\n",
    "    m=X_train.size\r\n",
    "    Theta_0=Theta\r\n",
    "    histoCost=[]\r\n",
    "    HistoTheta=[[],[]]\r\n",
    "    for i in range(steps):\r\n",
    "        #print(Theta_0)\r\n",
    "        #Calculo de la hypotesis\r\n",
    "        Hip=Theta_0[0]+X_train*Theta_0[1]\r\n",
    "        #Calculo de la diferencia\r\n",
    "        diff=Hip-Y_train\r\n",
    "        #El valor del costo J\r\n",
    "        cost=np.sum(diff**2)/(2*m) \r\n",
    "        histoCost.append(cost)\r\n",
    "        HistoTheta[0].append(Theta_0[0])\r\n",
    "        HistoTheta[1].append(Theta_0[1])\r\n",
    "        #Los nuevos thetas\r\n",
    "        Theta_0[0]=Theta_0[0]-(alpha/m)*np.sum(diff)\r\n",
    "        Theta_0[1]=Theta_0[1]-(alpha/m)*np.sum(diff*X_train)\r\n",
    "\r\n",
    "    return Theta_0,HistoTheta,histoCost"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#En este punto corremos la funcion sobre los datos\r\n",
    "pasos =100\r\n",
    "theta_0, HistoTheta,HistoCosto = GDLR(x,y,alpha=0.001,Theta=[0,0],steps=pasos)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "En las proximas gráficas podemos ver, la tasa de convergencia. Si usamos un $\\alpha$ mayor, o menor esa tasa se verá afectada. Tambíen podemos ver como cambian los parámetros $\\theta$ a medida que pasan las iteraciones. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig3, ax3 = plt.subplots(1,3,figsize=(12, 4))\r\n",
    "#Pasos\r\n",
    "p=np.linspace(0,100,len(HistoTheta[0]))\r\n",
    "ax3[0].plot(HistoCosto)\r\n",
    "ax3[0].set(ylabel='$J(\\theta_i)$')\r\n",
    "ax3[1].plot(HistoTheta[0],'--',color=\"gray\")\r\n",
    "ax3[1].set(ylabel=r'$\\theta_0$')\r\n",
    "ax3[2].plot(HistoTheta[1],'--', color=\"green\")\r\n",
    "ax3[2].set(ylabel=r'$\\theta_1$')\r\n",
    "fig3.tight_layout(pad=2.0)\r\n",
    "plt.plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig4, ax4 = plt.subplots(nrows=3, ncols=3,figsize=(15, 8))\r\n",
    "#ax4.set(xlabel=\"Horas\",ylabel=\"Nota\")\r\n",
    "#Usando un for \r\n",
    "for i in range(-3,6):\r\n",
    "    #Para producir los indices de los subplots\r\n",
    "    r, c = (i+3) // 3, (i) % 3\r\n",
    "    #Grafico los datos\r\n",
    "    ax4[r,c].clear()\r\n",
    "    ax4[r,c].plot(x, y,'x',color='red',label='Datos')\r\n",
    "    #Escojo un valor de theta\r\n",
    "    index = np.random.randint(0,pasos)\r\n",
    "    TH0=HistoTheta[0][index]\r\n",
    "    TH1=HistoTheta[1][index]\r\n",
    "    mse = HistoCosto[index]\r\n",
    "    #Linea\r\n",
    "    hip = lambda x : TH0+ TH1*x\r\n",
    "    #Hipotesis\r\n",
    "    ax4[r,c].plot(x,hip(x))\r\n",
    "    #Pongo los titulos\r\n",
    "    ax4[r,c].set_title(r'$\\theta_0$: {:.2f},$\\theta_1$: {:.2f}, J = {:.2f}'.format(TH0,TH1,mse))\r\n",
    "plt.tight_layout()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Grafiquemos\r\n",
    "fig2, ax2 = plt.subplots(figsize=(10, 6))\r\n",
    "ax2.set(xlabel=\"Horas\",ylabel=\"Nota\")\r\n",
    "# los valores de x e y\r\n",
    "hip = lambda x : theta_0[0]+ theta_0[1]*x\r\n",
    "ax2.plot(x, y,'x',color='red',label='Datos')\r\n",
    "ax2.plot(x, hip(x),label=r'$h_{\\theta}(x)$' )\r\n",
    "ax2.legend(fontsize=15)\r\n",
    "ax2.set_xlim([0.0, 10.0])\r\n",
    "ax2.set_ylim([0.0, 5.0])\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "ef4c800377bbf5e33c22b32317d08627756e48ff809c13c8c6fbaacd1e58e059"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}