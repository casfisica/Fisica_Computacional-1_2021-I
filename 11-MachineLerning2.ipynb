{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Learning (Aprendizage Automático)\r\n",
    "## Variables Múltiples\r\n",
    "\r\n",
    "Para estudiar la regresión lineal con múltiples variables(características) o _regresión lineal multivariante_, primero es necesario introducir la notación para ecuaciones en las que podemos tener cualquier número de variables de entrada. Suponga que ahora queremos refinar nuestro modelo que predice las notas de los estuduiantes, y anexamos a dicho modelo las características de _edad_ $E$, _promedio de notas_ $P$ , etc\r\n",
    "\r\n",
    "| H (Horas de estudio  Semanales) $x_1$  | E (Edad en años) $x_2$  | P (Promedio de  notas ponderado) $x_3$ | ... |  Nota |\r\n",
    "|:--------------------------------------:|:-----------------------:|:--------------------------------------:|:---:|:-----:|\r\n",
    "| 4.2                                    | 20                      | 3.5                                    |  .  | 2.3   |\r\n",
    "| 7.2                                    | 19                      | 4.0                                    |  .  | 2.6   |\r\n",
    "| 1.1                                    | 22                      | 3.1                                    |  .  | 0.1   |\r\n",
    "| 3.1                                    | 18                      | 3.7                                    |  .  | 1.1   |\r\n",
    "| 1.4                                    | 25                      | 4.4                                    |  .  | 0.8   |\r\n",
    "| . . .                                  | . . .                   | . . .                                  | ... | . . . |\r\n",
    "\r\n",
    "donde $m$ es el número de ejemplos de entrenamiento, $n$ el número de características (variables).\r\n",
    "Luego en ésta notación $x^{(i)}$ es la entrada (en este caso un vector de entradas) del i-ésimo ejemplo de entrenamiento y $x^{(i)}_j$ : Valor de la característica $j$ en el i-ésimo ejemplo de entrenamiento.\r\n",
    "\r\n",
    "La forma multivariable de la función de hipótesis lineal que acomoda estas múltiples características es la siguiente:\r\n",
    "\r\n",
    "$$h_\\theta(x) = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + \\theta_3 x_3 + \\cdots + \\theta_n x_n$$\r\n",
    "\r\n",
    "donde $x_0=1$. Usando la definición de multiplicación de matrices, la función de hipótesis multivariable se puede representar de manera concisa como:\r\n",
    "\r\n",
    "$$\r\n",
    "h_\\theta(x)=[ \\theta_0, \\theta_1, \\theta_2, \\theta_3, \\cdots , \\theta_n]\\begin{bmatrix}\r\n",
    "              x_{1} \\\\\r\n",
    "              x_{2} \\\\\r\n",
    "              \\vdots \\\\\r\n",
    "              x_{n}\r\n",
    "              \\end{bmatrix}\r\n",
    "              =\\theta^Tx\r\n",
    "$$\r\n",
    "\r\n",
    "Luego la función de costo se verá como :\r\n",
    "$$\r\n",
    "J(\\theta) = \\dfrac{1}{2m} \\sum_{i = 1}^m\\left(\\theta^Tx-y_{i}\\right)^2\r\n",
    "$$\r\n",
    "y el algoritmo de GD se transforma en:\r\n",
    "\r\n",
    "$$\r\n",
    "\\theta_j: = \\theta_j -\\frac{\\alpha}{m}\\sum_{i=1}^{m}(h(\\theta^{(i)}) - y^{(i)}).x^{(i)}_j\r\n",
    "$$\r\n",
    "\r\n",
    "__Ejemplo:__ En el ejemplo anterior suponga una caracteristica llamada _Ingresos mensuales domesticos_ que mide la entrada de dinero al grupo familiar del estudiante, y estamos interesados en usar ésta nueva información en nuestro modelo."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Importamos los modulos necesarios\r\n",
    "from random import choice\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Creamos un conjunto de datos\r\n",
    "N=300\r\n",
    "#Usando una semilla\r\n",
    "np.random.seed(1)\r\n",
    "#Inicializamos el contenedor y llenamos las variables\r\n",
    "x=np.array([np.zeros(N),np.zeros(N)])\r\n",
    "x[0] = 10*np.random.random(N)\r\n",
    "#LLenamos la segunda variable, usando una distribucion gamma\r\n",
    "shape, scale = 1., 2.  # mean=4, std=2*sqrt(2)\r\n",
    "x[1] = 908526.0*np.random.gamma(shape, scale, N)\r\n",
    "#Notas, donde anexamos la parte del ingreso cuadraticamente\r\n",
    "y = 0.4+0.2*x[0] +3*(1/(x[1].max()))*x[1]+ 0.4*np.random.randn(N)\r\n",
    "print('Normalización theta_2: ',3*(1/(x[1].max()))**2)\r\n",
    "# Grafiquemos\r\n",
    "fig, ax = plt.subplots(nrows=2,ncols=1,figsize=(10, 6))\r\n",
    "_ = ax[0].plot(x[0], y,'x',color='red',label='Datos')\r\n",
    "_ = ax[1].plot(x[1], y,'x',color='red',label='Datos')\r\n",
    "\r\n",
    "ax[0].set(xlabel=\"Horas\",ylabel=\"Nota\")\r\n",
    "ax[1].set(xlabel=\"Ingreso Munsual $\",ylabel=\"Nota\")\r\n",
    "\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora usemos las definiciones para actualizar la función de Descenso por Gradiente y usarla con multiples variables."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def GDLR(x,y,Theta,alpha=0.001,steps=100):\r\n",
    "    '''Gradient Descent \r\n",
    "        Args:\r\n",
    "            - Theta (array float): parámetros iniciales np.ones([1,2])\r\n",
    "            - X_train (numpy float arrays): Vector con los valores del rango de entrenamiento\r\n",
    "                Ejem:   x = np.array([np.zeros(N),np.zeros(N)])\r\n",
    "                        x[1] = 10*np.random.random(N)\r\n",
    "            - Y_train( numpy float array): Vector con los valores del dominio de entrenamiento \r\n",
    "            - alpha (float): learning rate\r\n",
    "            - steps (int): numero de pasos\r\n",
    "        Out:\r\n",
    "            - Valor de Theta actualizado a la ultima iteracion (array float)\r\n",
    "            - Historico del Thetha\r\n",
    "            - Historico del costo\r\n",
    "    '''\r\n",
    "    #Agregamos la columna correspondiente a $x_0$\r\n",
    "    Arreglo = []\r\n",
    "    Arreglo.append(np.ones(x.shape[1]))\r\n",
    "    for i in x:\r\n",
    "        Arreglo.append(i)\r\n",
    "        x_train= np.array(Arreglo)\r\n",
    "    #Transonemos para que \r\n",
    "    xMatrix = x_train.transpose()\r\n",
    "\r\n",
    "    #Numero de muestras\r\n",
    "    m=x.shape[1]\r\n",
    "    Theta_0=Theta.transpose()\r\n",
    "    histoCost=[]\r\n",
    "    HistoTheta=np.zeros(shape=(Theta_0.size,steps))\r\n",
    "\r\n",
    "    for i in range(steps):\r\n",
    "        #Calculo de la hypotesis\r\n",
    "        Hip = np.dot(xMatrix,Theta_0)\r\n",
    "        #Calculo de la diferencia\r\n",
    "        diff=Hip-y.transpose()\r\n",
    "        #El valor del costo J\r\n",
    "        cost=np.sum(diff**2)/(2*m) \r\n",
    "        histoCost.append(cost)\r\n",
    "        for indx, j in enumerate(HistoTheta):\r\n",
    "            j[i]=Theta_0[indx]\r\n",
    "        #Los nuevos thetas\r\n",
    "        Theta_0=Theta_0-(alpha/m)*np.dot(diff,xMatrix)\r\n",
    "        \r\n",
    "    return Theta_0,HistoTheta,histoCost"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Theta=np.array([0,0,0])\r\n",
    "alpha=0.01\r\n",
    "steps=10\r\n",
    "ThetaFinal ,HistoTheta,histoCost = GDLR(x,y,Theta,alpha=alpha,steps=steps)\r\n",
    "fig3, ax3 = plt.subplots(2,2,figsize=(12, 6))\r\n",
    "#Pasos\r\n",
    "p=np.linspace(0,100,len(HistoTheta[0]))\r\n",
    "ax3[0,0].plot(histoCost)\r\n",
    "ax3[0,0].set(ylabel=r'$J(\\theta_i)$')\r\n",
    "ax3[0,1].plot(HistoTheta[0],'--',color=\"gray\")\r\n",
    "ax3[0,1].set(ylabel=r'$\\theta_0$')\r\n",
    "ax3[1,0].plot(HistoTheta[1],'--', color=\"green\")\r\n",
    "ax3[1,0].set(ylabel=r'$\\theta_1$')\r\n",
    "ax3[1,1].plot(HistoTheta[2],'--', color=\"green\")\r\n",
    "ax3[1,1].set(ylabel=r'$\\theta_2$')\r\n",
    "\r\n",
    "fig3.tight_layout(pad=2.0)\r\n",
    "plt.plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "__Notas:__\r\n",
    "- Escalar las variables:\r\n",
    "    Una práctica común cuando se usa GD, es la de escalar las variables al intervalo (-1,1), o usar el escalamiento normal:\r\n",
    "    $$x'_i=\\frac{x_i-\\mu_i}{s_i}$$ \r\n",
    "    donde $\\mu_i$ es la media de la valores de entrenamiento $x_i$ y $s_i= x_{i,max}-x_{i,min}$ (tambien se puede usar la desivación estándar)\r\n",
    "- Cambiar el valor de $\\alpha$\r\n",
    "    Cuando no se tiene convergencia, o la convergencia esta tomando muchas iteraciones, es recomendable graficar $J(\\theta)$ Vs _Número de iteraciones_ y evalual la posibilidad de reducir el valor de $\\alpha$."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Escalamos la variable x^(2)\r\n",
    "xs=x.copy()\r\n",
    "s=x[1].max()-x[1].min()\r\n",
    "xs[1] = (x[1] - x[1].mean())/s\r\n",
    "\r\n",
    "Theta=np.array([0,0,0])\r\n",
    "alpha=0.01\r\n",
    "steps=1000\r\n",
    "ThetaFinal ,HistoTheta,histoCost = GDLR(xs,y,Theta,alpha=alpha,steps=steps)\r\n",
    "fig3, ax3 = plt.subplots(2,2,figsize=(12, 6))\r\n",
    "#Pasos\r\n",
    "p=np.linspace(0,100,len(HistoTheta[0]))\r\n",
    "ax3[0,0].plot(histoCost)\r\n",
    "ax3[0,0].set(ylabel=r'$J(\\theta_i)$')\r\n",
    "ax3[0,1].plot(HistoTheta[0],'--',color=\"gray\")\r\n",
    "ax3[0,1].set(ylabel=r'$\\theta_0$')\r\n",
    "ax3[1,0].plot(HistoTheta[1],'--', color=\"green\")\r\n",
    "ax3[1,0].set(ylabel=r'$\\theta_1$')\r\n",
    "ax3[1,1].plot(HistoTheta[2],'--', color=\"green\")\r\n",
    "ax3[1,1].set(ylabel=r'$\\theta_2$')\r\n",
    "\r\n",
    "fig3.tight_layout(pad=2.0)\r\n",
    "plt.plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Podemos mejorar nuestras características y la forma de nuestra función de hipótesis de un par de formas diferentes.\r\n",
    "\r\n",
    "Podemos combinar múltiples funciones en una. Por ejemplo, podemos combinar la variable de _ingresos mensuales_ con el _número de familiares en el hogar_ y así tener otra variable que puede ser más representativa para nuestro modelo.\r\n",
    "\r\n",
    "### Regresión polinomial\r\n",
    "La función de hipótesis no necesita ser lineal, pues si eso no se ajusta bien a los datos, en preincipio podemos cambiar el comportamiento o la curva de la función de hipótesis convirtiéndola en una función cuadrática, cúbica o de raiz cuadrada (o cualquier otra forma).\r\n",
    "\r\n",
    "Por ejemplo, si nuestra función de hipótesis es $h_{\\theta}(x) = \\theta_0 + \\theta_1x_1$, se puede crear funciones adicionales basadas en $x_1$ para obtener la función cuadrática $h_{\\theta}(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_1^ 2$.\r\n",
    "\r\n",
    "Una cosa importante a tener en cuenta es que si elige sus características de esta manera, la escala de las variables se vuelve muy importante.\r\n",
    "\r\n",
    "## Ecuacion Normal\r\n",
    "\r\n",
    "El descenso por gradiente proporciona una forma de minimizar la función de costo $J$, pero existe una segunda forma de hacerlo, esta vez realizando la minimización de forma explícita y sin recurrir a un algoritmo iterativo. En el método de la \"ecuación normal\", se encuentra el conjunto $\\theta$ que minimiza $J$ tomando explícitamente sus derivadas con respecto a las $\\theta_j$ y poniéndolas a cero. \r\n",
    "\r\n",
    "Comenzamos con la definición de $J$ (la última versión)\r\n",
    "\r\n",
    "$$\r\n",
    "J(\\theta) = \\dfrac{1}{2m} \\sum_{i = 1}^m\\left(\\theta^Tx-y_{i}\\right)^2\r\n",
    "$$\r\n",
    "\r\n",
    "Cuando el término $\\left(\\theta^Tx-y_{i}\\right)^2$ se suma en todas las muestras, podemos usar la notación matricial. Definiremos la \"matriz de diseño\" $X$ ($X$ mayúscula) como una matriz de $m$ filas, en la que cada fila es la $i$-ésima muestra (el vector x^{(i)}). Con esto, podemos reescribir el costo de mínimos cuadrados, reemplazando la suma explícita por la multiplicación de matrices:\r\n",
    "\r\n",
    "$$\r\n",
    "J(\\theta) = \\dfrac{1}{2m} \\left(X\\theta-y\\right)^T\\left(X\\theta-y\\right)\r\n",
    "$$\r\n",
    "\r\n",
    "Ahora, usando algunas identidades de transposición de matrices, podemos simplificar\r\n",
    "$$\r\n",
    "\\begin{equation}\r\n",
    "    \\begin{split}\r\n",
    "        J(\\theta)   & = ((X\\theta)^T-y^T)(X\\theta-y)\\\\\r\n",
    "                    & = (X\\theta)^T X\\theta-(X\\theta)^T y-y^T(X\\theta)+y^Ty)\\\\\r\n",
    "                    & = (X\\theta)^T X\\theta-2(X\\theta)^T y+y^Ty)\\\\\r\n",
    "    \\end{split}\r\n",
    "\\end{equation}\r\n",
    "$$\r\n",
    "\r\n",
    "Para encontrar dónde la función tiene un mínimo, derivamos por $\\theta$ e igualamos a $0$. \r\n",
    "$$\r\n",
    "\\frac{\\partial}{\\partial\\theta}J = 2X^TX\\theta-2X^Ty =0\r\n",
    "$$\r\n",
    "Luego asumiendo que $X^TX$ tiene inverza, concluimos que\r\n",
    "\r\n",
    "$$\\theta = (X^T X)^{- 1}X^Ty$$\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def Normal(X,y):    \r\n",
    "    '''Ecuación Normal \r\n",
    "    Args:\r\n",
    "        - X (numpy float arrays): Matriz de diseño\r\n",
    "        - y( numpy float array): Vector con los valores del dominio de entrenamiento \r\n",
    "        Out:\r\n",
    "            - Valor de Theta en un min\r\n",
    "    '''\r\n",
    "    return np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#lo primero que hacemos es crear la matriz de diseño X, con x del ejercicio anterior\r\n",
    "#Agregamos la columna correspondiente a $x_0$\r\n",
    "Arreglo = []\r\n",
    "Arreglo.append(np.ones(x.shape[1]))\r\n",
    "for i in x:\r\n",
    "    Arreglo.append(i)\r\n",
    "    x_train= np.array(Arreglo)\r\n",
    "#Transonemos para que \r\n",
    "MD = x_train.transpose()\r\n",
    "Theta_N=Normal(MD,y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Theta_N"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Con la ecuación normal, calcular la inversión tiene complejidad $\\mathcal{O}(n^3)$. Luego, si tenemos una gran cantidad de variables, la ecuación normal será lenta. En la práctica, cuando $n$ supera los $10.000$, podría ser un buen momento para pasar de una solución normal a un proceso iterativo."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "ef4c800377bbf5e33c22b32317d08627756e48ff809c13c8c6fbaacd1e58e059"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}